{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb1a886",
   "metadata": {},
   "source": [
    "### Bias in Data\n",
    "\n",
    "The goal of this assignment is to explore the concept of bias through data on Wikipedia articles - specifically, articles on political figures from a variety of countries. For this assignment, you will combine a dataset of Wikipedia articles with a dataset of country populations, and use a machine learning service called ORES to estimate the quality of each article.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72fdd47",
   "metadata": {},
   "source": [
    "### STEP 1: Data Acquisition\n",
    "The first step is getting the data, which lives in several different places.\n",
    "\n",
    "The Wikipedia politicians by country dataset can be found on Figshare. Read through the documentation for this repository, then download and unzip it to extract the data file, which is called page_data.csv.\n",
    "\n",
    "The population data is available in CSV format as WPDS_2020_data.csv. This dataset is drawn from the world population data sheet published by the Population Reference Bureau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5c1bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9748e",
   "metadata": {},
   "source": [
    "### Read politicians article data\n",
    "\n",
    " The Wikipedia politicians by country dataset is downloaded from Figshare. We download and unzip it to extract the data file, which is called page_data.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "931fd87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>country</th>\n",
       "      <th>rev_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Template:ZambiaProvincialMinisters</td>\n",
       "      <td>Zambia</td>\n",
       "      <td>235107991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bir I of Kanem</td>\n",
       "      <td>Chad</td>\n",
       "      <td>355319463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Template:Zimbabwe-politician-stub</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>391862046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Template:Uganda-politician-stub</td>\n",
       "      <td>Uganda</td>\n",
       "      <td>391862070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Template:Namibia-politician-stub</td>\n",
       "      <td>Namibia</td>\n",
       "      <td>391862409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 page   country     rev_id\n",
       "0  Template:ZambiaProvincialMinisters    Zambia  235107991\n",
       "1                      Bir I of Kanem      Chad  355319463\n",
       "2   Template:Zimbabwe-politician-stub  Zimbabwe  391862046\n",
       "3     Template:Uganda-politician-stub    Uganda  391862070\n",
       "4    Template:Namibia-politician-stub   Namibia  391862409"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = pd.read_csv('page_data.csv')\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c130347",
   "metadata": {},
   "source": [
    "### Read population data \n",
    "\n",
    "The population data is available in CSV format as WPDS_2020_data.csv. This dataset is drawn from the world population data sheet published by the Population Reference Bureau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05624f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>TimeFrame</th>\n",
       "      <th>Data (M)</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WORLD</td>\n",
       "      <td>WORLD</td>\n",
       "      <td>World</td>\n",
       "      <td>2019</td>\n",
       "      <td>7772.850</td>\n",
       "      <td>7772850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFRICA</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>Sub-Region</td>\n",
       "      <td>2019</td>\n",
       "      <td>1337.918</td>\n",
       "      <td>1337918000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NORTHERN AFRICA</td>\n",
       "      <td>NORTHERN AFRICA</td>\n",
       "      <td>Sub-Region</td>\n",
       "      <td>2019</td>\n",
       "      <td>244.344</td>\n",
       "      <td>244344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DZ</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>Country</td>\n",
       "      <td>2019</td>\n",
       "      <td>44.357</td>\n",
       "      <td>44357000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EG</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>Country</td>\n",
       "      <td>2019</td>\n",
       "      <td>100.803</td>\n",
       "      <td>100803000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              FIPS             Name        Type  TimeFrame  Data (M)  \\\n",
       "0            WORLD            WORLD       World       2019  7772.850   \n",
       "1           AFRICA           AFRICA  Sub-Region       2019  1337.918   \n",
       "2  NORTHERN AFRICA  NORTHERN AFRICA  Sub-Region       2019   244.344   \n",
       "3               DZ          Algeria     Country       2019    44.357   \n",
       "4               EG            Egypt     Country       2019   100.803   \n",
       "\n",
       "   Population  \n",
       "0  7772850000  \n",
       "1  1337918000  \n",
       "2   244344000  \n",
       "3    44357000  \n",
       "4   100803000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population = pd.read_csv(\"WPDS_2020_data.csv\")\n",
    "population.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4180481f",
   "metadata": {},
   "source": [
    "### STEP 2: Cleaning the Data\n",
    "Both page_data.csv and WPDS_2020_data.csv contain some rows that you will need to filter out and/or ignore when you combine the datasets in the next step. \n",
    "\n",
    "In the case of articles data, the dataset contains some page names that start with the string \"Template:\". These pages are not Wikipedia articles, and should not be included in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ed925f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article data set size before dropping articles that start with template: (47197, 3)\n",
      "article data set size after dropping articles that start with template: (46701, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>country</th>\n",
       "      <th>rev_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bir I of Kanem</td>\n",
       "      <td>Chad</td>\n",
       "      <td>355319463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Information Minister of the Palestinian Nation...</td>\n",
       "      <td>Palestinian Territory</td>\n",
       "      <td>393276188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Yos Por</td>\n",
       "      <td>Cambodia</td>\n",
       "      <td>393822005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Julius Gregr</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>395521877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Edvard Gregr</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>395526568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 page                country  \\\n",
       "1                                      Bir I of Kanem                   Chad   \n",
       "10  Information Minister of the Palestinian Nation...  Palestinian Territory   \n",
       "12                                            Yos Por               Cambodia   \n",
       "23                                       Julius Gregr         Czech Republic   \n",
       "24                                       Edvard Gregr         Czech Republic   \n",
       "\n",
       "       rev_id  \n",
       "1   355319463  \n",
       "10  393276188  \n",
       "12  393822005  \n",
       "23  395521877  \n",
       "24  395526568  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"article data set size before dropping articles that start with template:\", articles.shape)\n",
    "articles = articles[~articles['page'].str.startswith('Template:')]\n",
    "print(\"article data set size after dropping articles that start with template:\", articles.shape)\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04922219",
   "metadata": {},
   "source": [
    "Similarly, population data set contains some rows that provide cumulative regional population counts, rather than country-level counts. These rows are distinguished by having ALL CAPS values in the 'geography' field (e.g. AFRICA, OCEANIA). These rows won't match the country values in page_data.csv, but you will want to retain them (either in the original file, or a separate file) so that you can report coverage and quality by region in the analysis section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1b5907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population data set size before dropping geography field name with ALL CAPS (234, 6)\n",
      "population data set size after dropping geography field name with ALL CAPS (210, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"population data set size before dropping geography field name with ALL CAPS\", population.shape)\n",
    "population = population[~population['Name'].str.isupper()]\n",
    "print(\"population data set size after dropping geography field name with ALL CAPS\", population.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c992ac",
   "metadata": {},
   "source": [
    "### STEP 3 -  Getting Article quality predictions.\n",
    "\n",
    "Using the wikimedia API edpoints that connects to a machine learning algorithm called ORES, we obtain predictions for each of the articles listted in the articles data. \n",
    "\n",
    "In order to get article predictions for each article in the Wikipedia dataset, you will first need to read page_data.csv into Python, and then read through the dataset line by line, using the value of the rev_id column to make an API query.\n",
    "\n",
    "The ORES API expects a revision ID, which is the third column in the Wikipedia dataset, and a model, which is \"articlequality\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaab21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'https://github.com/poornima-muthukumar',\n",
    "    'From': 'muthupoo@uw.edu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6258204",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'https://ores.wikimedia.org/v3/scores/{project}/{revid}/{model}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f2fb6",
   "metadata": {},
   "source": [
    "### Example API call for single Revision ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2de4f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Dump {\n",
      "    \"enwiki\": {\n",
      "        \"models\": {\n",
      "            \"articlequality\": {\n",
      "                \"version\": \"0.8.2\"\n",
      "            }\n",
      "        },\n",
      "        \"scores\": {\n",
      "            \"521986779\": {\n",
      "                \"articlequality\": {\n",
      "                    \"score\": {\n",
      "                        \"prediction\": \"Stub\",\n",
      "                        \"probability\": {\n",
      "                            \"B\": 0.009908958962632771,\n",
      "                            \"C\": 0.009436325360461916,\n",
      "                            \"FA\": 0.0018057235542233237,\n",
      "                            \"GA\": 0.0026249279565224385,\n",
      "                            \"Start\": 0.04946106501006811,\n",
      "                            \"Stub\": 0.9267629991560914\n",
      "                        }\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Prediction Stub\n"
     ]
    }
   ],
   "source": [
    "parameters = {'project' : 'enwiki',\n",
    "              'model'   : 'articlequality',\n",
    "              'revid'   : '521986779'\n",
    "              }    \n",
    "call = requests.get(endpoint.format(**parameters), headers=headers)\n",
    "response = call.json()\n",
    "\n",
    "print(\"JSON Dump\", json.dumps(response, indent=4))\n",
    "print(\"Prediction\", response['enwiki']['scores']['521986779']['articlequality']['score']['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9718f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_call(rev_id):\n",
    "    parameters = {'project' : 'enwiki',\n",
    "                      'model'   : 'articlequality',\n",
    "                      'revid'   : rev_id\n",
    "                      }  \n",
    "    call = requests.get(endpoint.format(**parameters), headers=headers)\n",
    "    response = call.json()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f154f30",
   "metadata": {},
   "source": [
    "We read the articles csv downloaded earlier row by and row and for each revision id we query the ORES API and parse the json response to retrieve the quality of the article.\n",
    "\n",
    "We write the result of revision id to quality mapping in a csv file, first by converting it to pandas data frame.\n",
    "\n",
    "This step takes a while to run as it queries the API for roughly ~47K ids. We enclose the code in a try catch block to catch API calls for which we do not get a response. We also write the revids for which the quality is missing into a csv file called missing_prediction_revids. \n",
    "\n",
    "This individual API call for each revision id takes a really long time to run. Instead we can use the bulk API call that returns result for 100 revision id at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089863a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception while calling ORES API for revision id 516633096\n",
      "Exception while calling ORES API for revision id 550682925\n",
      "Exception while calling ORES API for revision id 627547024\n",
      "Exception while calling ORES API for revision id 636911471\n",
      "Exception while calling ORES API for revision id 669987106\n",
      "Exception while calling ORES API for revision id 671484594\n",
      "Exception while calling ORES API for revision id 680981536\n",
      "Exception while calling ORES API for revision id 684023803\n",
      "Exception while calling ORES API for revision id 684023859\n",
      "Exception while calling ORES API for revision id 696608092\n",
      "Exception while calling ORES API for revision id 698572327\n",
      "Exception while calling ORES API for revision id 699260156\n",
      "Exception while calling ORES API for revision id 703773782\n",
      "Exception while calling ORES API for revision id 706204833\n",
      "Exception while calling ORES API for revision id 706810694\n",
      "Exception while calling ORES API for revision id 708482569\n",
      "Exception while calling ORES API for revision id 708813010\n",
      "Exception while calling ORES API for revision id 709508670\n",
      "Exception while calling ORES API for revision id 710135228\n",
      "Exception while calling ORES API for revision id 710311600\n",
      "Exception while calling ORES API for revision id 710715953\n",
      "Exception while calling ORES API for revision id 711224007\n",
      "Exception while calling ORES API for revision id 711288191\n",
      "Exception while calling ORES API for revision id 711513274\n",
      "Exception while calling ORES API for revision id 712411818\n",
      "Exception while calling ORES API for revision id 712872338\n",
      "Exception while calling ORES API for revision id 712872421\n",
      "Exception while calling ORES API for revision id 712872473\n",
      "Exception while calling ORES API for revision id 712872531\n",
      "Exception while calling ORES API for revision id 712873183\n",
      "Exception while calling ORES API for revision id 712873308\n",
      "Exception while calling ORES API for revision id 712873386\n",
      "Exception while calling ORES API for revision id 712878000\n",
      "Exception while calling ORES API for revision id 712878267\n",
      "Exception while calling ORES API for revision id 712878343\n",
      "Exception while calling ORES API for revision id 712878396\n",
      "Exception while calling ORES API for revision id 712881543\n",
      "Exception while calling ORES API for revision id 712881676\n",
      "Exception while calling ORES API for revision id 712881741\n",
      "Exception while calling ORES API for revision id 712881882\n",
      "Exception while calling ORES API for revision id 712889562\n",
      "Exception while calling ORES API for revision id 712889594\n",
      "Exception while calling ORES API for revision id 712889683\n",
      "Exception while calling ORES API for revision id 712889781\n",
      "Exception while calling ORES API for revision id 712889809\n",
      "Exception while calling ORES API for revision id 712891291\n",
      "Exception while calling ORES API for revision id 712891354\n",
      "Exception while calling ORES API for revision id 712891378\n",
      "Exception while calling ORES API for revision id 712891476\n",
      "Exception while calling ORES API for revision id 713368646\n",
      "Exception while calling ORES API for revision id 713381693\n",
      "Exception while calling ORES API for revision id 714352602\n",
      "Exception while calling ORES API for revision id 715273866\n",
      "Exception while calling ORES API for revision id 715457941\n",
      "Exception while calling ORES API for revision id 715534283\n",
      "Exception while calling ORES API for revision id 715978328\n",
      "Exception while calling ORES API for revision id 717536136\n",
      "Exception while calling ORES API for revision id 717891895\n",
      "Exception while calling ORES API for revision id 717917231\n",
      "Exception while calling ORES API for revision id 717927381\n",
      "Exception while calling ORES API for revision id 718090116\n",
      "Exception while calling ORES API for revision id 719342595\n",
      "Exception while calling ORES API for revision id 719521006\n",
      "Exception while calling ORES API for revision id 719581803\n",
      "Exception while calling ORES API for revision id 719981739\n",
      "Exception while calling ORES API for revision id 720054719\n",
      "Exception while calling ORES API for revision id 720154872\n",
      "Exception while calling ORES API for revision id 720164068\n",
      "Exception while calling ORES API for revision id 720356159\n",
      "Exception while calling ORES API for revision id 720688837\n",
      "Exception while calling ORES API for revision id 720856841\n",
      "Exception while calling ORES API for revision id 720924221\n",
      "Exception while calling ORES API for revision id 720953589\n",
      "Exception while calling ORES API for revision id 720959757\n",
      "Exception while calling ORES API for revision id 720993927\n",
      "Exception while calling ORES API for revision id 721106120\n",
      "Exception while calling ORES API for revision id 721509220\n",
      "Exception while calling ORES API for revision id 721856824\n",
      "Exception while calling ORES API for revision id 723082034\n",
      "Exception while calling ORES API for revision id 723308478\n",
      "Exception while calling ORES API for revision id 724052271\n",
      "Exception while calling ORES API for revision id 726600165\n",
      "Exception while calling ORES API for revision id 726639434\n",
      "Exception while calling ORES API for revision id 726805368\n",
      "Exception while calling ORES API for revision id 730950147\n",
      "Exception while calling ORES API for revision id 734957625\n",
      "Exception while calling ORES API for revision id 735966681\n",
      "Exception while calling ORES API for revision id 736660951\n",
      "Exception while calling ORES API for revision id 737123608\n",
      "Exception while calling ORES API for revision id 738480520\n",
      "Exception while calling ORES API for revision id 738514517\n",
      "Exception while calling ORES API for revision id 738984692\n",
      "Exception while calling ORES API for revision id 739115995\n",
      "Exception while calling ORES API for revision id 740126873\n",
      "Exception while calling ORES API for revision id 743770874\n",
      "Exception while calling ORES API for revision id 744643166\n",
      "Exception while calling ORES API for revision id 745915558\n",
      "Exception while calling ORES API for revision id 746758535\n",
      "Exception while calling ORES API for revision id 747688056\n",
      "Exception while calling ORES API for revision id 747922835\n",
      "Exception while calling ORES API for revision id 748601806\n",
      "Exception while calling ORES API for revision id 749326717\n",
      "Exception while calling ORES API for revision id 749389032\n",
      "Exception while calling ORES API for revision id 749546552\n",
      "Exception while calling ORES API for revision id 749699147\n",
      "Exception while calling ORES API for revision id 749705073\n",
      "Exception while calling ORES API for revision id 750849372\n",
      "Exception while calling ORES API for revision id 753191526\n",
      "Exception while calling ORES API for revision id 754739349\n",
      "Exception while calling ORES API for revision id 756697478\n",
      "Exception while calling ORES API for revision id 757313957\n",
      "Exception while calling ORES API for revision id 757961591\n",
      "Exception while calling ORES API for revision id 758706289\n",
      "Exception while calling ORES API for revision id 760194556\n",
      "Exception while calling ORES API for revision id 760195376\n",
      "Exception while calling ORES API for revision id 761821348\n",
      "Exception while calling ORES API for revision id 762816132\n",
      "Exception while calling ORES API for revision id 763245610\n",
      "Exception while calling ORES API for revision id 763558111\n",
      "Exception while calling ORES API for revision id 764019224\n",
      "Exception while calling ORES API for revision id 764191934\n",
      "Exception while calling ORES API for revision id 764911200\n",
      "Exception while calling ORES API for revision id 765228329\n",
      "Exception while calling ORES API for revision id 765489047\n",
      "Exception while calling ORES API for revision id 765662083\n",
      "Exception while calling ORES API for revision id 765717347\n",
      "Exception while calling ORES API for revision id 766101638\n",
      "Exception while calling ORES API for revision id 768013050\n",
      "Exception while calling ORES API for revision id 768871687\n",
      "Exception while calling ORES API for revision id 768889609\n",
      "Exception while calling ORES API for revision id 771213598\n",
      "Exception while calling ORES API for revision id 771642775\n",
      "Exception while calling ORES API for revision id 772176512\n",
      "Exception while calling ORES API for revision id 772567736\n",
      "Exception while calling ORES API for revision id 773323473\n",
      "Exception while calling ORES API for revision id 773355081\n",
      "Exception while calling ORES API for revision id 773601556\n",
      "Exception while calling ORES API for revision id 774023957\n",
      "Exception while calling ORES API for revision id 774293013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception while calling ORES API for revision id 777163201\n",
      "Exception while calling ORES API for revision id 778388481\n",
      "Exception while calling ORES API for revision id 778618827\n",
      "Exception while calling ORES API for revision id 779101752\n",
      "Exception while calling ORES API for revision id 779135011\n",
      "Exception while calling ORES API for revision id 779409209\n",
      "Exception while calling ORES API for revision id 779899001\n",
      "Exception while calling ORES API for revision id 779954797\n",
      "Exception while calling ORES API for revision id 779957437\n",
      "Exception while calling ORES API for revision id 780854503\n",
      "Exception while calling ORES API for revision id 781072934\n",
      "Exception while calling ORES API for revision id 781427254\n"
     ]
    }
   ],
   "source": [
    "%%script false \n",
    "\n",
    "revids = []\n",
    "predictions = []\n",
    "missing_prediction_revids = []\n",
    "\n",
    "for row in articles.iterrows():\n",
    "    try:\n",
    "        rev_id = row[1]['rev_id']\n",
    "        response = api_call(rev_id)\n",
    "        revids.append(rev_id)\n",
    "        predictions.append(response['enwiki']['scores'][str(rev_id)]['articlequality']['score']['prediction'])\n",
    "    except:\n",
    "        missing_prediction_revids.append(row[1]['rev_id'])\n",
    "        print(\"Exception while calling ORES API for revision id\", row[1]['rev_id'])\n",
    "\n",
    "\n",
    "print(\"Writing to file revid to quality mapping\")\n",
    "revid_quality = pd.dataFrame([revids, predictions]).T\n",
    "revid_quality.columns = ['revid', 'quality']\n",
    "revid_quality.to_csv('wikipedia-politician-article-quality.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"Writing to file rev_ids missing prediction\")\n",
    "missing_prediction_revids.to_csv('missing_prediction_revids.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "179904a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_ids = articles['rev_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc670623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_api_call(rev_ids):\n",
    "    \n",
    "    bulk_endpoint = 'https://ores.wikimedia.org/v3/scores/{project}/?models={model}&revids={revids}'\n",
    "    parameters = {'project' : 'enwiki',\n",
    "                  'model'   : 'articlequality',\n",
    "                  'revids'  : '|'.join(str(x) for x in rev_ids)\n",
    "                  }  \n",
    "    \n",
    "    call = requests.get(bulk_endpoint.format(**parameters), headers=headers)\n",
    "    response = call.json()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2def1ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Quality for Revision Id 516633096\n",
      "Missing Quality for Revision Id 550682925\n",
      "Missing Quality for Revision Id 627547024\n",
      "Missing Quality for Revision Id 636911471\n",
      "Missing Quality for Revision Id 669987106\n",
      "Missing Quality for Revision Id 671484594\n",
      "Missing Quality for Revision Id 680981536\n",
      "Missing Quality for Revision Id 684023803\n",
      "Missing Quality for Revision Id 684023859\n",
      "Missing Quality for Revision Id 696608092\n",
      "Missing Quality for Revision Id 698572327\n",
      "Missing Quality for Revision Id 699260156\n",
      "Missing Quality for Revision Id 703773782\n",
      "Missing Quality for Revision Id 706204833\n",
      "Missing Quality for Revision Id 706810694\n",
      "Missing Quality for Revision Id 708482569\n",
      "Missing Quality for Revision Id 708813010\n",
      "Missing Quality for Revision Id 709508670\n",
      "Missing Quality for Revision Id 710135228\n",
      "Missing Quality for Revision Id 710311600\n",
      "Missing Quality for Revision Id 710715953\n",
      "Missing Quality for Revision Id 711224007\n",
      "Missing Quality for Revision Id 711288191\n",
      "Missing Quality for Revision Id 711513274\n",
      "Missing Quality for Revision Id 712411818\n",
      "Missing Quality for Revision Id 712872338\n",
      "Missing Quality for Revision Id 712872421\n",
      "Missing Quality for Revision Id 712872473\n",
      "Missing Quality for Revision Id 712872531\n",
      "Missing Quality for Revision Id 712873183\n",
      "Missing Quality for Revision Id 712873308\n",
      "Missing Quality for Revision Id 712873386\n",
      "Missing Quality for Revision Id 712878000\n",
      "Missing Quality for Revision Id 712878267\n",
      "Missing Quality for Revision Id 712878343\n",
      "Missing Quality for Revision Id 712878396\n",
      "Missing Quality for Revision Id 712881543\n",
      "Missing Quality for Revision Id 712881676\n",
      "Missing Quality for Revision Id 712881741\n",
      "Missing Quality for Revision Id 712881882\n",
      "Missing Quality for Revision Id 712889562\n",
      "Missing Quality for Revision Id 712889594\n",
      "Missing Quality for Revision Id 712889683\n",
      "Missing Quality for Revision Id 712889781\n",
      "Missing Quality for Revision Id 712889809\n",
      "Missing Quality for Revision Id 712891291\n",
      "Missing Quality for Revision Id 712891354\n",
      "Missing Quality for Revision Id 712891378\n",
      "Missing Quality for Revision Id 712891476\n",
      "Missing Quality for Revision Id 713368646\n",
      "Missing Quality for Revision Id 713381693\n",
      "Missing Quality for Revision Id 714352602\n",
      "Missing Quality for Revision Id 715273866\n",
      "Missing Quality for Revision Id 715457941\n",
      "Missing Quality for Revision Id 715534283\n",
      "Missing Quality for Revision Id 715978328\n",
      "Missing Quality for Revision Id 717536136\n",
      "Missing Quality for Revision Id 717891895\n",
      "Missing Quality for Revision Id 717917231\n",
      "Missing Quality for Revision Id 717927381\n",
      "Missing Quality for Revision Id 718090116\n",
      "Missing Quality for Revision Id 719342595\n",
      "Missing Quality for Revision Id 719521006\n",
      "Missing Quality for Revision Id 719581803\n",
      "Missing Quality for Revision Id 719981739\n",
      "Missing Quality for Revision Id 720054719\n",
      "Missing Quality for Revision Id 720154872\n",
      "Missing Quality for Revision Id 720164068\n",
      "Missing Quality for Revision Id 720356159\n",
      "Missing Quality for Revision Id 720688837\n",
      "Missing Quality for Revision Id 720856841\n",
      "Missing Quality for Revision Id 720924221\n",
      "Missing Quality for Revision Id 720953589\n",
      "Missing Quality for Revision Id 720959757\n",
      "Missing Quality for Revision Id 720993927\n",
      "Missing Quality for Revision Id 721106120\n",
      "Missing Quality for Revision Id 721509220\n",
      "Missing Quality for Revision Id 721856824\n",
      "Missing Quality for Revision Id 723082034\n",
      "Missing Quality for Revision Id 723308478\n",
      "Missing Quality for Revision Id 724052271\n",
      "Missing Quality for Revision Id 726600165\n",
      "Missing Quality for Revision Id 726639434\n",
      "Missing Quality for Revision Id 726805368\n",
      "Missing Quality for Revision Id 730950147\n",
      "Missing Quality for Revision Id 734957625\n",
      "Missing Quality for Revision Id 735966681\n",
      "Missing Quality for Revision Id 736660951\n",
      "Missing Quality for Revision Id 737123608\n",
      "Missing Quality for Revision Id 738480520\n",
      "Missing Quality for Revision Id 738514517\n",
      "Missing Quality for Revision Id 738984692\n",
      "Missing Quality for Revision Id 739115995\n",
      "Missing Quality for Revision Id 740126873\n",
      "Missing Quality for Revision Id 743770874\n",
      "Missing Quality for Revision Id 744643166\n",
      "Missing Quality for Revision Id 745915558\n",
      "Missing Quality for Revision Id 746758535\n",
      "Missing Quality for Revision Id 747688056\n",
      "Missing Quality for Revision Id 747922835\n",
      "Missing Quality for Revision Id 748601806\n",
      "Missing Quality for Revision Id 749326717\n",
      "Missing Quality for Revision Id 749389032\n",
      "Missing Quality for Revision Id 749546552\n",
      "Missing Quality for Revision Id 749699147\n",
      "Missing Quality for Revision Id 749705073\n",
      "Missing Quality for Revision Id 750849372\n",
      "Missing Quality for Revision Id 753191526\n",
      "Missing Quality for Revision Id 754739349\n",
      "Missing Quality for Revision Id 756697478\n",
      "Missing Quality for Revision Id 757313957\n",
      "Missing Quality for Revision Id 757961591\n",
      "Missing Quality for Revision Id 758706289\n",
      "Missing Quality for Revision Id 760194556\n",
      "Missing Quality for Revision Id 760195376\n",
      "Missing Quality for Revision Id 761821348\n",
      "Missing Quality for Revision Id 762816132\n",
      "Missing Quality for Revision Id 763245610\n",
      "Missing Quality for Revision Id 763558111\n",
      "Missing Quality for Revision Id 764019224\n",
      "Missing Quality for Revision Id 764191934\n",
      "Missing Quality for Revision Id 764911200\n",
      "Missing Quality for Revision Id 765228329\n",
      "Missing Quality for Revision Id 765489047\n",
      "Missing Quality for Revision Id 765662083\n",
      "Missing Quality for Revision Id 765717347\n",
      "Missing Quality for Revision Id 766101638\n",
      "Missing Quality for Revision Id 768013050\n",
      "Missing Quality for Revision Id 768871687\n",
      "Missing Quality for Revision Id 771213598\n",
      "Missing Quality for Revision Id 771642775\n",
      "Missing Quality for Revision Id 772176512\n",
      "Missing Quality for Revision Id 772567736\n",
      "Missing Quality for Revision Id 773323473\n",
      "Missing Quality for Revision Id 773355081\n",
      "Missing Quality for Revision Id 773601556\n",
      "Missing Quality for Revision Id 774023957\n",
      "Missing Quality for Revision Id 774293013\n",
      "Missing Quality for Revision Id 777163201\n",
      "Missing Quality for Revision Id 778388481\n",
      "Missing Quality for Revision Id 778618827\n",
      "Missing Quality for Revision Id 779101752\n",
      "Missing Quality for Revision Id 779135011\n",
      "Missing Quality for Revision Id 779409209\n",
      "Missing Quality for Revision Id 779899001\n",
      "Missing Quality for Revision Id 779954797\n",
      "Missing Quality for Revision Id 779957437\n",
      "Missing Quality for Revision Id 781072934\n",
      "Missing Quality for Revision Id 781427254\n",
      "Missing Quality for Revision Id 782170063\n",
      "Missing Quality for Revision Id 782508289\n",
      "Missing Quality for Revision Id 783382630\n",
      "Missing Quality for Revision Id 784558743\n",
      "Missing Quality for Revision Id 786721553\n",
      "Missing Quality for Revision Id 787181453\n",
      "Missing Quality for Revision Id 787353922\n",
      "Missing Quality for Revision Id 787391918\n",
      "Missing Quality for Revision Id 787398581\n",
      "Missing Quality for Revision Id 787453368\n",
      "Missing Quality for Revision Id 788037971\n",
      "Missing Quality for Revision Id 788289620\n",
      "Missing Quality for Revision Id 788310383\n",
      "Missing Quality for Revision Id 788722110\n",
      "Missing Quality for Revision Id 788844423\n",
      "Missing Quality for Revision Id 788884593\n",
      "Missing Quality for Revision Id 789080627\n",
      "Missing Quality for Revision Id 789150159\n",
      "Missing Quality for Revision Id 789262504\n",
      "Missing Quality for Revision Id 789266166\n",
      "Missing Quality for Revision Id 789271422\n",
      "Missing Quality for Revision Id 789281061\n",
      "Missing Quality for Revision Id 789285351\n",
      "Missing Quality for Revision Id 789285762\n",
      "Missing Quality for Revision Id 789286413\n",
      "Missing Quality for Revision Id 789444415\n",
      "Missing Quality for Revision Id 789736824\n",
      "Missing Quality for Revision Id 789816636\n",
      "Missing Quality for Revision Id 790028876\n",
      "Missing Quality for Revision Id 790147995\n",
      "Missing Quality for Revision Id 790410710\n",
      "Missing Quality for Revision Id 791429003\n",
      "Missing Quality for Revision Id 791866288\n",
      "Missing Quality for Revision Id 792184018\n",
      "Missing Quality for Revision Id 792400552\n",
      "Missing Quality for Revision Id 792440851\n",
      "Missing Quality for Revision Id 792857662\n",
      "Missing Quality for Revision Id 792933603\n",
      "Missing Quality for Revision Id 793280504\n",
      "Missing Quality for Revision Id 794311724\n",
      "Missing Quality for Revision Id 794409906\n",
      "Missing Quality for Revision Id 794768894\n",
      "Missing Quality for Revision Id 794866848\n",
      "Missing Quality for Revision Id 794990759\n",
      "Missing Quality for Revision Id 795545202\n",
      "Missing Quality for Revision Id 795588781\n",
      "Missing Quality for Revision Id 796744496\n",
      "Missing Quality for Revision Id 796758810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Quality for Revision Id 797311794\n",
      "Missing Quality for Revision Id 797902544\n",
      "Missing Quality for Revision Id 798073295\n",
      "Missing Quality for Revision Id 798615865\n",
      "Missing Quality for Revision Id 798841317\n",
      "Missing Quality for Revision Id 798865216\n",
      "Missing Quality for Revision Id 798891126\n",
      "Missing Quality for Revision Id 798945915\n",
      "Missing Quality for Revision Id 798952867\n",
      "Missing Quality for Revision Id 799036953\n",
      "Missing Quality for Revision Id 799263010\n",
      "Missing Quality for Revision Id 799300265\n",
      "Missing Quality for Revision Id 799540668\n",
      "Missing Quality for Revision Id 799880073\n",
      "Missing Quality for Revision Id 799933274\n",
      "Missing Quality for Revision Id 799981363\n",
      "Missing Quality for Revision Id 800072571\n",
      "Missing Quality for Revision Id 800144885\n",
      "Missing Quality for Revision Id 800233652\n",
      "Missing Quality for Revision Id 800299837\n",
      "Missing Quality for Revision Id 800471126\n",
      "Missing Quality for Revision Id 800574730\n",
      "Missing Quality for Revision Id 800707821\n",
      "Missing Quality for Revision Id 800802122\n",
      "Missing Quality for Revision Id 801177435\n",
      "Missing Quality for Revision Id 801306872\n",
      "Missing Quality for Revision Id 801536406\n",
      "Missing Quality for Revision Id 801563949\n",
      "Missing Quality for Revision Id 801566979\n",
      "Missing Quality for Revision Id 801638735\n",
      "Missing Quality for Revision Id 801696196\n",
      "Missing Quality for Revision Id 801835819\n",
      "Missing Quality for Revision Id 802148439\n",
      "Missing Quality for Revision Id 802302246\n",
      "Missing Quality for Revision Id 802445176\n",
      "Missing Quality for Revision Id 802475516\n",
      "Missing Quality for Revision Id 802515929\n",
      "Missing Quality for Revision Id 802765034\n",
      "Missing Quality for Revision Id 802904214\n",
      "Missing Quality for Revision Id 802937732\n",
      "Missing Quality for Revision Id 803413615\n",
      "Missing Quality for Revision Id 803484030\n",
      "Missing Quality for Revision Id 803784385\n",
      "Missing Quality for Revision Id 804057093\n",
      "Missing Quality for Revision Id 804118645\n",
      "Missing Quality for Revision Id 804158476\n",
      "Missing Quality for Revision Id 804473605\n",
      "Missing Quality for Revision Id 804697858\n",
      "Missing Quality for Revision Id 804791599\n",
      "Missing Quality for Revision Id 804941978\n",
      "Missing Quality for Revision Id 804945338\n",
      "Missing Quality for Revision Id 804946658\n",
      "Missing Quality for Revision Id 805031334\n",
      "Missing Quality for Revision Id 805041930\n",
      "Missing Quality for Revision Id 805211553\n",
      "Missing Quality for Revision Id 805420467\n",
      "Missing Quality for Revision Id 805517427\n",
      "Missing Quality for Revision Id 805697623\n",
      "Missing Quality for Revision Id 805788048\n",
      "Missing Quality for Revision Id 805936041\n",
      "Missing Quality for Revision Id 805970954\n",
      "Missing Quality for Revision Id 805993160\n",
      "Missing Quality for Revision Id 806030859\n",
      "Missing Quality for Revision Id 806179522\n",
      "Missing Quality for Revision Id 806223655\n",
      "Missing Quality for Revision Id 806372496\n",
      "Missing Quality for Revision Id 806403304\n",
      "Missing Quality for Revision Id 806542084\n",
      "Missing Quality for Revision Id 806695084\n",
      "Missing Quality for Revision Id 806708318\n",
      "Missing Quality for Revision Id 806811023\n",
      "Missing Quality for Revision Id 807000274\n",
      "Missing Quality for Revision Id 807161510\n",
      "Missing Quality for Revision Id 807196681\n",
      "Missing Quality for Revision Id 807336308\n",
      "Missing Quality for Revision Id 807367030\n",
      "Missing Quality for Revision Id 807367166\n",
      "Missing Quality for Revision Id 807479587\n",
      "Missing Quality for Revision Id 807484325\n",
      "Writing to file revid to quality mapping\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'dataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yl/klvpctlj1gxbttytmtblttpw0000gn/T/ipykernel_50201/2377715598.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Writing to file revid to quality mapping\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mrevid_quality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrevids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mrevid_quality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'revid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quality'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mrevid_quality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wikipedia-politician-article-quality.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'dataFrame'"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "end = 50\n",
    "\n",
    "revids = []\n",
    "predictions = []\n",
    "missing_prediction_revids = []\n",
    "\n",
    "for t in range(math.ceil(len(rev_ids)/50)):\n",
    "    ids = rev_ids[start:end]\n",
    "    response = bulk_api_call(ids)\n",
    "    for revid in ids:\n",
    "        if not response['enwiki']['scores'][str(revid)]['articlequality'].get('score') is None:\n",
    "            revids.append(revid)\n",
    "            \n",
    "            predictions.append(response['enwiki']['scores'][str(revid)]['articlequality']['score']['prediction'])\n",
    "        else:\n",
    "            missing_prediction_revids.append(revid)\n",
    "            print(\"Missing Quality for Revision Id\", revid)\n",
    "            \n",
    "    start+=50\n",
    "    end+=50 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a8567d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file revid to quality mapping\n",
      "Writing to file revid missing prediction\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing to file revid to quality mapping\")\n",
    "revid_quality = pd.DataFrame([revids, predictions]).T\n",
    "revid_quality.columns = ['revision_id', 'article_quality_est']\n",
    "revid_quality.to_csv('wikipedia-politician-article-quality.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"Writing to file revid missing prediction\")\n",
    "revid_missing_prediction = pd.DataFrame([missing_prediction_revids]).T\n",
    "revid_missing_prediction.columns = ['revid']\n",
    "revid_missing_prediction.to_csv('missing_prediction_revids.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf5c8c",
   "metadata": {},
   "source": [
    "### STEP 4 - Combining the data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a55bde5",
   "metadata": {},
   "source": [
    "Some processing of the data will be necessary! In particular, you'll need to - after retrieving and including the ORES data for each article - merge the wikipedia data and population data together. Both have fields containing country names for just that purpose. After merging the data, you'll invariably run into entries which cannot be merged. Either the population dataset does not have an entry for the equivalent Wikipedia country, or vise versa.\n",
    "\n",
    "Please remove any rows that do not have matching data, and output them to a CSV file called:\n",
    "wp_wpds_countries-no_match.csv\n",
    "\n",
    "Consolidate the remaining data into a single CSV file called:\n",
    "wp_wpds_politicians_by_country.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9bb37ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_population = articles.merge(population, how='outer', left_on='country', right_on='Name')\n",
    "\n",
    "missing_population_or_article = articles_population.loc[(articles_population['country'].isnull() | \n",
    "                                        articles_population['Name'].isnull())]\n",
    "\n",
    "missing_population_or_article.to_csv('wp_wpds_countries-no_match.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a9dfbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged  = articles.merge(population, how='inner', left_on='country', right_on='Name')\n",
    "\n",
    "del final_merged['FIPS']\n",
    "del final_merged['Name']\n",
    "del final_merged['Type']\n",
    "del final_merged['TimeFrame']\n",
    "del final_merged['Population']\n",
    "\n",
    "final_merged = final_merged.rename(index=str, columns={\"page\": \"article_name\", \"rev_id\": \"revision_id\", \"Data (M)\":\"population\"})\n",
    "\n",
    "final_merged_quality = final_merged.merge(revid_quality, on='revision_id')\n",
    "final_merged_quality.to_csv('wp_wpds_politicians_by_country.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234bb09d",
   "metadata": {},
   "source": [
    "###  STEP 5 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1913ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
